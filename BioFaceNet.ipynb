{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BioFaceNet.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1VYLVTh2pC-jnWBlJ1pHm6ljj6qDXrCea",
      "authorship_tag": "ABX9TyPkQ8RjAReXFTEjDRcTtmx7",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/UCLAbrucequ/BioFaceNet-Pytorch-repo/blob/main/BioFaceNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ksU6Qe-v2I3",
        "outputId": "8928ee6b-2ffb-4826-cd0c-21ba0986abe6"
      },
      "source": [
        "from google.colab import drive \n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S890S-kaS96h"
      },
      "source": [
        "#import libraries and modules\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.io\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.utils as vutils\n",
        "\n",
        "\n",
        "\n",
        "from scipy.io import loadmat\n",
        "from torchvision.datasets import CelebA\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn import preprocessing"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AWu7eKs4aO60"
      },
      "source": [
        "# Root directory for dataset\n",
        "dataroot = \"/content/drive/MyDrive/VMG/celeba\"\n",
        "\n",
        "# Number of workers for dataloader\n",
        "workers = 2\n",
        "\n",
        "# Batch size during training\n",
        "batch_size = 64\n",
        "\n",
        "# Number of Total images\n",
        "n_images = 50765\n",
        "\n",
        "# Spatial size of training images. All images will be resized to this\n",
        "#   size using a transformer.\n",
        "image_size = 64\n",
        "\n",
        "# Number of channels in the training images. For color images this is 3\n",
        "nc = 3\n",
        "\n",
        "# Size of z latent vector (i.e. size of generator input)\n",
        "nz = 100\n",
        "\n",
        "# Size of feature maps in generator\n",
        "ngf = 64\n",
        "\n",
        "# Size of feature maps in discriminator\n",
        "ndf = 64\n",
        "\n",
        "# Number of training epochs\n",
        "num_epochs = 200\n",
        "\n",
        "# Learning rate for optimizers\n",
        "lr = 1e-5\n",
        "\n",
        "# Beta1 hyperparam for Adam optimizers\n",
        "beta1 = 0.5\n",
        "\n",
        "# Number of GPUs available. Use 0 for CPU mode.\n",
        "ngpu = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KcWKoMF0UmQx",
        "outputId": "095ce039-e2aa-4d8f-f16d-0a356434221e"
      },
      "source": [
        "#check device\n",
        "\n",
        "# Decide which device we want to run on\n",
        "device = torch.device(\"cuda\" if (torch.cuda.is_available()) else \"cpu\")\n",
        "\n",
        "print(device)\n",
        "#The program is running on cuda"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cpu\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nbUWvd2pwA9I"
      },
      "source": [
        "#unzip the file\n",
        "#only run once\n",
        "\n",
        "!unzip -u \"/content/drive/MyDrive/VMG/img_align_celeba.zip\" -d \"/content/drive/MyDrive/VMG/celeba\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7kKJt_jnnNo6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k0lEgiX2lk08"
      },
      "source": [
        "#Define transformations for the training set, resize the image, centercrop, and convert it to tensors\n",
        "train_transformations = transforms.Compose([\n",
        "    transforms.Resize(image_size),\n",
        "    transforms.CenterCrop(image_size),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))\n",
        "])\n",
        "\n",
        "#Load the training set\n",
        "train_set = CelebA(root='/content/drive/MyDrive/VMG', split = 'train' , transform=train_transformations, download=False)\n",
        "\n",
        "#Create a loader for the training set\n",
        "train_loader = torch.utils.data.DataLoader(train_set,batch_size=batch_size,shuffle=True,num_workers=workers)\n",
        "\n",
        "\n",
        "#Load the validation set\n",
        "valid_set = CelebA(root='/content/drive/MyDrive/VMG', split='valid', transform= train_transformations, download=False)\n",
        "\n",
        "#Load the test data\n",
        "test_set = CelebA(root = '/content/drive/MyDrive/VMG', split = 'test', transform= train_transformations, download=False)\n",
        "\n",
        "\n",
        "#Create a test dataloader for the test set\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_set, batch_size = batch_size, shuffle=True, num_workers=workers)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "deoeoJ7KAY2s",
        "outputId": "e6212c7a-f884-4c75-9777-9ef6e17a2f18"
      },
      "source": [
        "print(train_set)\n",
        "print(valid_set)\n",
        "print(test_set)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset CelebA\n",
            "    Number of datapoints: 162770\n",
            "    Root location: /content/drive/MyDrive/VMG\n",
            "    Target type: ['attr']\n",
            "    Split: train\n",
            "    StandardTransform\n",
            "Transform: Compose(\n",
            "               Resize(size=64, interpolation=bilinear, max_size=None, antialias=None)\n",
            "               CenterCrop(size=(64, 64))\n",
            "               ToTensor()\n",
            "               Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
            "           )\n",
            "Dataset CelebA\n",
            "    Number of datapoints: 19867\n",
            "    Root location: /content/drive/MyDrive/VMG\n",
            "    Target type: ['attr']\n",
            "    Split: valid\n",
            "    StandardTransform\n",
            "Transform: Compose(\n",
            "               Resize(size=64, interpolation=bilinear, max_size=None, antialias=None)\n",
            "               CenterCrop(size=(64, 64))\n",
            "               ToTensor()\n",
            "               Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
            "           )\n",
            "Dataset CelebA\n",
            "    Number of datapoints: 19962\n",
            "    Root location: /content/drive/MyDrive/VMG\n",
            "    Target type: ['attr']\n",
            "    Split: test\n",
            "    StandardTransform\n",
            "Transform: Compose(\n",
            "               Resize(size=64, interpolation=bilinear, max_size=None, antialias=None)\n",
            "               CenterCrop(size=(64, 64))\n",
            "               ToTensor()\n",
            "               Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
            "           )\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12JgdYnplksJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        },
        "outputId": "4a0d1241-daee-4297-d3b0-8af572245d0d"
      },
      "source": [
        "#Plot some training samples\n",
        "dataiter = iter(train_loader)\n",
        "images, labels  = dataiter.next()\n",
        "plt.figure(figsize=(8,8))\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Training Images\")\n",
        "\n",
        "#print(np.transpose(vutils.make_grid(dataiter[0].to(device)[:1], padding=2, normalize=True).cpu(),(0,1,2)).shape)\n",
        "plt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:64], padding=2, normalize=True).cpu(),(1,2,0)))\n",
        "# print([torch.min(images), torch.max(images)])"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-ba9244061c08>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m#print(np.transpose(vutils.make_grid(dataiter[0].to(device)[:1], padding=2, normalize=True).cpu(),(0,1,2)).shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_grid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;31m# print([torch.min(images), torch.max(images)])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'real_batch' is not defined"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        },
        "id": "91TvGWWZiDtM",
        "outputId": "ad6e6a58-4ca7-4348-e031-3df4f6963103"
      },
      "source": [
        "print(\"Image batch dimensions:\", images.shape)\n",
        "print(\"Image label dimensions:\", labels.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-b48334f5277e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Image batch dimensions:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Image label dimensions:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'images' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oRcPrkzekZdF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G2MrxJnv7K8j"
      },
      "source": [
        "def setup():\n",
        "\n",
        "  cmf = loadmat('/content/drive/MyDrive/VMG/Matlab Files/rgbCMF.mat');\n",
        "\n",
        "\t#PCA model for camera sensitivities\n",
        "  [mu, PC, EVpca] = CameraSensitivityPCA(cmf);\n",
        "\n",
        "  LightVectorSize = np.single(15);\n",
        "  wavelength = np.single(33);\n",
        "  bSize = np.single(2);\n",
        "\n",
        "\n",
        "  illF = loadmat('/content/drive/MyDrive/VMG/Matlab Files/illF.mat');\n",
        "  illF_array = illF['illF'];\n",
        "  illF = np.reshape(illF_array, [1,1,33,12]);\n",
        "\n",
        "  illumDmeasured = loadmat('/content/drive/MyDrive/VMG/Matlab Files/illumDmeasured.mat');\n",
        "  illumDmeasured = illumDmeasured['illumDmeasured'];\n",
        "  illumDmeasured_array = np.transpose(illumDmeasured);\n",
        "  illumDmeasured = np.reshape(illumDmeasured_array,[1,1,33,22]);\n",
        "\n",
        "\t#A\n",
        "  illumA = loadmat('/content/drive/MyDrive/VMG/Matlab Files/illumA.mat');\n",
        "  illumA_array = illumA['illumA'];\n",
        "  illumA = illumA_array/(np.sum(illumA_array));\n",
        "\n",
        "\t#D\n",
        "  illumDNorm = np.single(np.full((1,1, 33,22),0));\n",
        "  for i in range(0, 22):\n",
        "    illumDNorm[0,0,:,i] = illumDmeasured[0,0,:,i]/np.sum(illumDmeasured[0,0,:,i]);\n",
        "\n",
        "\t#F\n",
        "  illumFNorm = np.single(np.full((1,1, 33,22),0));\n",
        "  for i in range(0,12):\n",
        "    illumFNorm[0,0,:,i] = illF[0,0,:,i]/np.sum(illF[0,0,:,i]);\n",
        "\n",
        "def CameraSensitivityPCA(cmf):\n",
        "  X = np.zeros((99,28))\n",
        "  Y = np.zeros((99,28))\n",
        "\n",
        "\n",
        "  redS = cmf['rgbCMF'][0,0]\n",
        "  greenS = cmf['rgbCMF'][0,1]\n",
        "  blueS = cmf['rgbCMF'][0,2]\n",
        "\n",
        "  for i in range (0,28,1):\n",
        "    Y[0:33,i] = redS[:,i]/sum(redS[:,i])\n",
        "    Y[33:66,i] = greenS[:,i]/sum(greenS[:,i])\n",
        "    Y[66:99,i] = blueS[:,i]/sum(blueS[:,i])\n",
        "\n",
        "  PC = loadmat('/content/drive/MyDrive/VMG/Matlab Files/PC.mat')\n",
        "  PC = PC['PC']\n",
        "\n",
        "  mu = loadmat('/content/drive/MyDrive/VMG/Matlab Files/mu.mat')\n",
        "  mu = mu['mu']\n",
        "\n",
        "  EVpca = loadmat('/content/drive/MyDrive/VMG/Matlab Files/EVpca.mat')\n",
        "  EVpca = EVpca['EVpca']\n",
        "\n",
        "  return [mu, PC, EVpca]\n",
        "\n",
        "setup();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BDwBeoZE7K3a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c47eb829-f2d2-4fdd-99cb-28533db56ab4"
      },
      "source": [
        "nfilters = np.single([32,64,128,256,512])\n",
        "nlayers = np.size(nfilters)\n",
        "print(nlayers)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZMdqqyia7KzZ"
      },
      "source": [
        "#Define class Unit\n",
        "# Class Unit contains a convolutional layer, a batch normalization layer, and a Relu layer\n",
        "\n",
        "class Unit(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels):\n",
        "    super(Unit, self).__init__()\n",
        "\n",
        "    self.conv = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size = 3, stride = 1, padding = 1)\n",
        "    self.bn = nn.BatchNorm2d(num_features = out_channels)\n",
        "    self.relu = nn.ReLU()\n",
        "\n",
        "  def forward(self, input):\n",
        "    output = self.conv(input)\n",
        "    output = self.bn(output)\n",
        "    output = self.relu(output)\n",
        "\n",
        "\n",
        "    return output\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZXGMemDVvSb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "430QS-m77KvZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "outputId": "496fbce7-1fe6-4755-9369-ec23ba735980"
      },
      "source": [
        "#Encoder & Decoder\n",
        "\n",
        "\n",
        "#TODO:\n",
        "  # X_skip should be a tensor\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "  def __init__(self, num_classes = 4):\n",
        "    super(Net, self).__init__()\n",
        "\n",
        "\n",
        "  #Encoder Layers\n",
        "\n",
        "    #First loop\n",
        "    self.unit1 = Unit(in_channels=3, out_channels=nfilters[0])\n",
        "    self.unit2 = Unit(in_channels=nfilters[0], out_channels=nfilters[0])\n",
        "    self.unit3 = Unit(in_channels=nfilters[0], out_channels=nfilters[0])\n",
        "\n",
        "    self.x_skip[0] = self.unit3\n",
        "\n",
        "    self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "    #Second loop\n",
        "    self.unit4 = Unit(in_channels=nfilters[0], out_channels=nfilters[1])\n",
        "    self.unit5 = Unit(in_channels=nfilters[1], out_channels=nfilters[1])\n",
        "    self.untt6 = Unit(in_channels=nfilters[1], out_channels=nfilters[1])\n",
        "\n",
        "    self.x_skip[1] = self.unit6\n",
        "\n",
        "    self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "\n",
        "    #Third loop\n",
        "    self.unit7 = Unit(in_channels=nfilters[1], out_channels=nfilters[2])\n",
        "    self.unit8 = Unit(in_channels=nfilters[2], out_channels=nfilters[2])\n",
        "    self.unit9 = Unit(in_channels=nfilters[2], out_channels=nfilters[2])\n",
        "\n",
        "    self.x_skip[2] = self.unit9\n",
        "\n",
        "    self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "    #Encoder4\n",
        "    self.unit10 = Unit(in_channels=nfilters[2], out_channels=nfilters[3])\n",
        "    self.unit11 = Unit(in_channels=nfilters[3], out_channels=nfilters[3])\n",
        "    self.unit12 = Unit(in_channels=nfilters[3], out_channels=nfilters[3])\n",
        "\n",
        "    self.x_skip[3] = self.unit12\n",
        "\n",
        "    self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "    #Last loop\n",
        "    self.unit13 = Unit(in_channels=nfilters[3], out_channels=nfilters[4])\n",
        "    self.unit14 = Unit(in_channels=nfilters[4], out_channels=nfilters[4])\n",
        "    self.unit15 = Unit(in_channels=nfilters[4], out_channels=nfilters[4])\n",
        "    self.y = self.unit15\n",
        "\n",
        "\n",
        "\n",
        "    self.encoder1 = nn.Sequential(self.unit1, self.unit2, self.unit3)\n",
        "    self.encoder2 = nn.Sequential(self.pool1, self.unit4, self.unit5, self.unit6)\n",
        "    self.encoder3 = nn.Sequential(self.pool2, self.unit7, self.unit8, self.unit9)\n",
        "    self.encoder4 = nn.Sequential(self.pool3, self.unit10, self.unit11, self.unit12)\n",
        "    self.encoder5 = nn.Sequential(self.pool4, self.unit13, self.unit14, self.unit15)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    #self.encoder = nn.Sequential(self.unit1, self.unit2, self.unit3, self.pool1, self.unit4, self.unit5, self.unit6\n",
        "                                 #,self.pool2, self.unit7, self.unit8, self.unit9, self.pool3, self.unit10, self.unit11, self.unit12, self.pool4,\n",
        "                                 #self.unit13, self.unit14, self.unit15)\n",
        "    \n",
        "\n",
        "\n",
        "  #Decoder Layers\n",
        "\n",
        "    #i == 4\n",
        "    self.upsample1 = nn.Upsample()\n",
        "    #cat in forward\n",
        "\n",
        "    self.dunit1 = Unit(in_channels=nfilters[3]+nfilters[4], out_channels=nfilters[3])\n",
        "    self.dunit2 = Unit(in_channels=nfilters[3], out_channels=nfilters[3])\n",
        "    self.dunit3 = Unit(in_channels=nfilters[3], out_channels=nfilters[3])\n",
        "\n",
        "\n",
        "    #i == 3\n",
        "    self.upsample2 = nn.Upsample()\n",
        "\n",
        "    #cat in forward\n",
        "    self.dunit4 = Unit(in_channels=nfilters[2]+nfilters[3], out_channels=nfilters[2])\n",
        "    self.dunit5 = Unit(in_channels=nfilters[2], out_channels=nfilters[2])\n",
        "    self.dunit6 = Unit(in_channels=nfilters[2], out_channels=nfilters[2])\n",
        "\n",
        "\n",
        "    #i == 2\n",
        "    self.upsample3 = nn.Upsample()\n",
        "\n",
        "\n",
        "    #cat in forward\n",
        "    self.dunit7 = Unit(in_channels=nfilters[1]+nfilters[2], out_channels=nfilters[1])\n",
        "    self.dunit8 = Unit(in_channels=nfilters[1], out_channels=nfilters[1])\n",
        "    self.dunit9 = Unit(in_channels=nfilters[1], out_channels=nfilters[1])\n",
        "\n",
        "\n",
        "\n",
        "    #i == 1\n",
        "    self.upsample4 = nn.Upsample()\n",
        "\n",
        "\n",
        "    #cat in forward\n",
        "    self.dunit10 = Unit(in_channels=nfilters[0]+nfilters[1], out_channels=nfilters[0])\n",
        "    self.dunit11 = Unit(in_channels=nfilters[0], out_channels=nfilters[0])\n",
        "    self.dunit12 = Unit(in_channels=nfilters[0], out_channels=nfilters[0])\n",
        "\n",
        "\n",
        "\n",
        "    #Final prediction \n",
        "    self.dfinal = Unit(in_channels=nfilters[0], out_channels=1)\n",
        "    \n",
        "\n",
        "    self.dseq1 = nn.Sequential(self.dunit1, self.dunit2, self.dunit3)\n",
        "    self.dseq2 = nn.Sequential(self.dunit4, self.dunit5, self.dunit6)\n",
        "    self.dseq3 = nn.Sequential(self.dunit7, self.dunit8, self.dunit9)\n",
        "    self.dseq4 = nn.Sequential(self.dunit10, self.dunit11, self.dunit12)\n",
        "    self.doubleConv3 = nn.Sequential(self.unit11, self.unit12)\n",
        "    self.doubleConv2 = nn.Sequential(self.unit8, self.unit9)\n",
        "    self.doubleConv1 = nn.Sequential(self.unit5, self.unit6)\n",
        "    self.doubleConv0 = nn.Sequential(self.unit2, self.unit3)\n",
        "\n",
        "\n",
        "          \n",
        "\n",
        "      \n",
        "\n",
        "  def forward(self, input):\n",
        "\n",
        "    #Encoder\n",
        "\n",
        "    x_skip[0] = self.encoder1(input)\n",
        "    x_skip[1] = self.encoder2(x_skip[0])\n",
        "    x_skip[2] = self.encoder3(x_skip[1])\n",
        "    x_skip[3] = self.encoder4(x_skip[2])\n",
        "    y = self.encoder5(x_skip[5])\n",
        "\n",
        "\n",
        "    #Multiple Decoders\n",
        "\n",
        "    for c in range(1, num_classes+1): # c = 1, 2, ... , num_classes\n",
        "      #cat dim = 2\n",
        "      for i in range(nlayers-1, 0, -1):\n",
        "        if i == nlayers-1:\n",
        "          x = self.upsample1(y)\n",
        "          x = torch.cat((x, x_skip[i-1]), dim = 2)\n",
        "          x = self.dseq1(x)\n",
        "          x = self.doubleConv3(x)\n",
        "        elif i == nlayers-2:\n",
        "          x = self.upsample1(x)\n",
        "          x = torch.cat((x, x_skip[i-1]), dim = 2)\n",
        "          x = self.dseq2(x)\n",
        "          x = self.doubleConv2(x)\n",
        "        elif i == nlayers-3:\n",
        "          x = self.upsample1(x)\n",
        "          x = torch.cat((x,x_skip[i-1]), dim = 2)\n",
        "          x = self.dseq3(x)\n",
        "          x = self.doubleConv1(x)\n",
        "        elif i ==nlayers-4:\n",
        "          x = self.upsample1(x)\n",
        "          x = torch.cat((x, x_skip[i-1]), dim = 2)\n",
        "          x = self.dseq4(x)\n",
        "          x = self.doubleConv0(x)\n",
        "      \n",
        "      #Final Prediction\n",
        "      x = self.dfinal(x)\n",
        "\n",
        "      if c == 1:\n",
        "        z = x\n",
        "      else:\n",
        "        z = torch.cat((z,x), dim = 2)\n",
        "\n",
        "    x = z\n",
        "\n",
        "\n",
        "    #Fully Connected\n",
        "    fc1 = nn.Linear()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    return [x, y]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#TODO:\n",
        "#     Define the Model\n",
        "#     Set the parameters\n",
        "model = Net().to(device)\n",
        "\n",
        "print(model)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-52871e5f29d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;31m#     Define the Model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[0;31m#     Set the parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-52871e5f29d5>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, num_classes)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m#First loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munit1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUnit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_channels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_channels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnfilters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munit2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUnit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_channels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnfilters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_channels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnfilters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munit3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUnit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_channels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnfilters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_channels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnfilters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-29a2bc80c75e>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, in_channels, out_channels)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mUnit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_channels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0min_channels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_channels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout_channels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBatchNorm2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout_channels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReLU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, in_channels, out_channels, kernel_size, stride, padding, dilation, groups, bias, padding_mode, device, dtype)\u001b[0m\n\u001b[1;32m    430\u001b[0m         super(Conv2d, self).__init__(\n\u001b[1;32m    431\u001b[0m             \u001b[0min_channels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_channels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdilation_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 432\u001b[0;31m             False, _pair(0), groups, bias, padding_mode, **factory_kwargs)\n\u001b[0m\u001b[1;32m    433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, bias, padding_mode, device, dtype)\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             self.weight = Parameter(torch.empty(\n\u001b[0;32m--> 132\u001b[0;31m                 (out_channels, in_channels // groups, *kernel_size), **factory_kwargs))\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_channels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfactory_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: empty() received an invalid combination of arguments - got (tuple, dtype=NoneType, device=NoneType), but expected one of:\n * (tuple of ints size, *, tuple of names names, torch.memory_format memory_format, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (tuple of ints size, *, torch.memory_format memory_format, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l2N2xw7j1G22"
      },
      "source": [
        "# Function CNN\n",
        "def CNN(images, nfilters, nclass, LightVectorSize,bSize):\n",
        "  [x, y] = "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}